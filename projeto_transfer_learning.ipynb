{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendendo sobre Transfer Learning\n",
        "\n",
        "Este projeto tem por finalidade aplicar os conceitos aprendidos sobre Transfer Learning. A base do estudo seguiu o projeto desenvolvido por kylemath, sendo utilizado a base de imagens desenvolvida e parte do treinamento realizado.\n",
        "\n",
        "Desenvolver este projeto proporcionou aplicar os conceitos sobre sistemas operacionais, programação python, machine learning e deep learning. Compreender os ganhos em produtividade do uso de Transfer Learning ao aplicar a técnica e avaliar os resultados com objetos conhecidos.\n",
        "\n",
        "## Tecnologias utilizadas\n",
        "\n",
        "- keras\n",
        "- tensorflow\n",
        "- numpy\n",
        "- matplotlib\n",
        "\n",
        "## Referências\n",
        "\n",
        "[Ensina ai](https://medium.com/ensina-ai/tutorial-transfer-learning-3972cac5e9b5)\n",
        "\n",
        "[Adaptado de kylemath](https://colab.research.google.com/github/kylemath/ml4a-guides/blob/master/notebooks/transfer-learning.ipynb#scrollTo=3p-OjhDPYoZm)\n",
        "\n",
        "Dataset: [caltech edu](https://data.caltech.edu/records/mzrjq-6wc02)"
      ],
      "metadata": {
        "id": "lMlFM_okOVvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yh5rXPsyN7V3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "#if using Theano with GPU\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Extraindo pasta com a base das imagens.\"\n",
        "!tar -xzf 101_ObjectCategories.tar.gz\n",
        "!rm 101_ObjectCategories.tar.gz\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-eixvRLOI2V",
        "outputId": "d6b117d2-9366-4c15-c1b5-013182557b69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraindo pasta com a base das imagens.\n",
            "101_ObjectCategories  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = '101_ObjectCategories'\n",
        "exclude = ['BACKGROUND_Google', 'Motorbikes', 'airplanes', 'Faces_easy', 'Faces']\n",
        "train_split, val_split = 0.7, 0.15\n",
        "\n",
        "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
        "categories = [c for c in categories if c not in [os.path.join(root, e) for e in exclude]]\n",
        "\n",
        "print(categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KRUoXCZOOVq",
        "outputId": "1020af94-07ca-4c34-ab56-059b88c5da4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['101_ObjectCategories/kangaroo', '101_ObjectCategories/butterfly', '101_ObjectCategories/minaret', '101_ObjectCategories/crocodile_head', '101_ObjectCategories/ant', '101_ObjectCategories/metronome', '101_ObjectCategories/ewer', '101_ObjectCategories/lamp', '101_ObjectCategories/stop_sign', '101_ObjectCategories/car_side', '101_ObjectCategories/bass', '101_ObjectCategories/binocular', '101_ObjectCategories/elephant', '101_ObjectCategories/joshua_tree', '101_ObjectCategories/trilobite', '101_ObjectCategories/ketch', '101_ObjectCategories/scorpion', '101_ObjectCategories/grand_piano', '101_ObjectCategories/windsor_chair', '101_ObjectCategories/buddha', '101_ObjectCategories/saxophone', '101_ObjectCategories/revolver', '101_ObjectCategories/lobster', '101_ObjectCategories/laptop', '101_ObjectCategories/hawksbill', '101_ObjectCategories/platypus', '101_ObjectCategories/garfield', '101_ObjectCategories/pigeon', '101_ObjectCategories/crab', '101_ObjectCategories/strawberry', '101_ObjectCategories/scissors', '101_ObjectCategories/pyramid', '101_ObjectCategories/wheelchair', '101_ObjectCategories/llama', '101_ObjectCategories/euphonium', '101_ObjectCategories/beaver', '101_ObjectCategories/pagoda', '101_ObjectCategories/gramophone', '101_ObjectCategories/chair', '101_ObjectCategories/ibis', '101_ObjectCategories/sea_horse', '101_ObjectCategories/cougar_face', '101_ObjectCategories/menorah', '101_ObjectCategories/umbrella', '101_ObjectCategories/dolphin', '101_ObjectCategories/accordion', '101_ObjectCategories/water_lilly', '101_ObjectCategories/barrel', '101_ObjectCategories/crocodile', '101_ObjectCategories/brain', '101_ObjectCategories/dalmatian', '101_ObjectCategories/dollar_bill', '101_ObjectCategories/brontosaurus', '101_ObjectCategories/cannon', '101_ObjectCategories/wild_cat', '101_ObjectCategories/panda', '101_ObjectCategories/octopus', '101_ObjectCategories/lotus', '101_ObjectCategories/soccer_ball', '101_ObjectCategories/Leopards', '101_ObjectCategories/camera', '101_ObjectCategories/sunflower', '101_ObjectCategories/mayfly', '101_ObjectCategories/tick', '101_ObjectCategories/nautilus', '101_ObjectCategories/snoopy', '101_ObjectCategories/schooner', '101_ObjectCategories/wrench', '101_ObjectCategories/bonsai', '101_ObjectCategories/anchor', '101_ObjectCategories/hedgehog', '101_ObjectCategories/inline_skate', '101_ObjectCategories/mandolin', '101_ObjectCategories/stapler', '101_ObjectCategories/dragonfly', '101_ObjectCategories/rhino', '101_ObjectCategories/cellphone', '101_ObjectCategories/cup', '101_ObjectCategories/flamingo', '101_ObjectCategories/okapi', '101_ObjectCategories/rooster', '101_ObjectCategories/flamingo_head', '101_ObjectCategories/cougar_body', '101_ObjectCategories/emu', '101_ObjectCategories/crayfish', '101_ObjectCategories/chandelier', '101_ObjectCategories/watch', '101_ObjectCategories/stegosaurus', '101_ObjectCategories/electric_guitar', '101_ObjectCategories/gerenuk', '101_ObjectCategories/yin_yang', '101_ObjectCategories/headphone', '101_ObjectCategories/ceiling_fan', '101_ObjectCategories/pizza', '101_ObjectCategories/helicopter', '101_ObjectCategories/ferry', '101_ObjectCategories/starfish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to load image and return it and input vector\n",
        "def get_image(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return img, x"
      ],
      "metadata": {
        "id": "zfQZE4uRRfhC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for c, category in enumerate(categories):\n",
        "    images = [os.path.join(dp, f) for dp, dn, filenames\n",
        "              in os.walk(category) for f in filenames\n",
        "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "    for img_path in images:\n",
        "        img, x = get_image(img_path)\n",
        "        data.append({'x':np.array(x[0]), 'y':c})\n",
        "\n",
        "# count the number of classes\n",
        "num_classes = len(categories)"
      ],
      "metadata": {
        "id": "ZdBsWWUmRaEo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "v7iMKiwQRpuh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_val = int(train_split * len(data))\n",
        "idx_test = int((train_split + val_split) * len(data))\n",
        "train = data[:idx_val]\n",
        "val = data[idx_val:idx_test]\n",
        "test = data[idx_test:]"
      ],
      "metadata": {
        "id": "BEbITHNARtXy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
        "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
        "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD0ecSoXRx2R",
        "outputId": "1e66646b-cbff-451e-a079-874f00af1198"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19, 92, 39, 75, 85, 13, 69, 66, 86, 40, 65, 25, 43, 15, 52, 52, 49, 86, 68, 42, 83, 10, 78, 3, 2, 86, 57, 48, 14, 87, 40, 31, 61, 68, 46, 86, 94, 40, 18, 83, 35, 87, 71, 85, 92, 44, 85, 15, 94, 6, 24, 55, 69, 25, 37, 47, 66, 18, 47, 95, 83, 47, 44, 71, 43, 85, 39, 87, 18, 20, 58, 12, 85, 8, 3, 59, 57, 59, 89, 14, 85, 2, 32, 2, 38, 87, 72, 45, 66, 16, 21, 62, 53, 10, 23, 81, 79, 15, 33, 63, 41, 6, 96, 75, 53, 93, 32, 50, 6, 66, 49, 83, 3, 9, 1, 85, 39, 64, 78, 86, 68, 26, 83, 79, 11, 61, 53, 13, 34, 71, 6, 52, 37, 71, 17, 93, 9, 9, 13, 37, 30, 17, 24, 75, 94, 86, 15, 25, 6, 14, 57, 78, 12, 90, 23, 16, 41, 89, 88, 39, 83, 40, 38, 35, 74, 94, 21, 16, 61, 39, 6, 6, 51, 21, 36, 51, 86, 1, 18, 87, 36, 58, 37, 86, 11, 64, 95, 96, 26, 83, 40, 58, 68, 41, 63, 42, 15, 2, 39, 64, 83, 1, 78, 43, 35, 15, 56, 61, 46, 40, 94, 1, 16, 11, 94, 95, 50, 90, 68, 75, 37, 75, 6, 34, 50, 84, 37, 42, 24, 84, 92, 36, 17, 9, 56, 37, 14, 92, 16, 57, 63, 15, 75, 95, 11, 77, 74, 66, 13, 6, 95, 50, 13, 14, 32, 30, 34, 33, 10, 23, 82, 69, 24, 54, 9, 12, 68, 76, 85, 21, 87, 84, 82, 51, 15, 28, 90, 92, 39, 88, 2, 21, 81, 11, 56, 95, 49, 23, 87, 63, 44, 67, 33, 34, 33, 43, 41, 39, 6, 84, 95, 5, 93, 2, 82, 15, 37, 17, 31, 73, 88, 96, 31, 20, 67, 9, 69, 24, 22, 75, 57, 22, 35, 10, 68, 57, 75, 27, 17, 87, 76, 76, 45, 86, 58, 24, 67, 88, 35, 89, 32, 66, 61, 22, 47, 1, 23, 17, 8, 14, 96, 27, 16, 67, 39, 54, 52, 24, 87, 74, 38, 6, 95, 32, 4, 53, 14, 85, 10, 88, 86, 28, 66, 62, 42, 41, 95, 45, 46, 86, 75, 69, 39, 10, 74, 52, 86, 9, 59, 42, 11, 66, 55, 59, 85, 87, 95, 85, 24, 3, 12, 74, 47, 17, 68, 81, 68, 86, 14, 1, 7, 41, 96, 49, 87, 63, 40, 2, 49, 86, 9, 8, 14, 49, 42, 68, 96, 86, 21, 57, 66, 44, 41, 43, 24, 45, 29, 19, 11, 0, 13, 77, 86, 54, 16, 14, 15, 13, 64, 20, 17, 74, 88, 83, 58, 61, 17, 68, 52, 28, 23, 88, 33, 28, 95, 89, 79, 50, 34, 86, 45, 94, 16, 59, 45, 86, 15, 90, 94, 41, 0, 18, 74, 59, 68, 85, 57, 0, 73, 32, 53, 23, 45, 23, 9, 68, 85, 9, 53, 43, 7, 74, 63, 67, 53, 85, 45, 15, 89, 9, 36, 14, 90, 94, 1, 40, 19, 76, 58, 59, 85, 35, 22, 72, 87, 54, 12, 16, 93, 13, 44, 7, 38, 14, 86, 18, 37, 2, 74, 59, 8, 76, 75, 65, 3, 2, 96, 70, 42, 83, 1, 40, 29, 32, 28, 41, 52, 39, 22, 84, 73, 95, 53, 36, 19, 42, 24, 19, 51, 41, 59, 68, 36, 64, 57, 74, 15, 31, 59, 1, 43, 86, 16, 54, 3, 19, 68, 60, 79, 27, 85, 11, 91, 80, 49, 24, 21, 59, 77, 67, 22, 86, 52, 51, 32, 58, 38, 43, 29, 31, 45, 71, 33, 30, 5, 38, 47, 96, 79, 86, 7, 94, 59, 16, 43, 59, 65, 67, 26, 78, 30, 56, 46, 33, 62, 82, 22, 94, 28, 41, 15, 12, 34, 72, 42, 24, 59, 33, 55, 69, 76, 16, 48, 12, 81, 94, 33, 55, 9, 82, 27, 90, 14, 57, 42, 85, 15, 57, 42, 19, 74, 4, 96, 74, 65, 74, 84, 19, 78, 36, 2, 54, 73, 54, 52, 24, 10, 78, 48, 85, 88, 7, 59, 86, 33, 53, 90, 34, 31, 32, 86, 95, 14, 32, 8, 28, 94, 50, 13, 90, 42, 27, 30, 55, 84, 33, 61, 32, 33, 1, 74, 69, 63, 78, 49, 19, 86, 30, 27, 31, 18, 13, 1, 30, 45, 86, 66, 6, 13, 86, 6, 17, 88, 21, 60, 6, 59, 48, 90, 43, 96, 31, 94, 95, 32, 48, 28, 46, 83, 44, 7, 23, 42, 41, 13, 24, 25, 86, 5, 0, 86, 32, 87, 57, 79, 42, 80, 59, 21, 24, 81, 51, 21, 52, 34, 92, 17, 74, 37, 44, 49, 76, 60, 41, 1, 76, 27, 86, 86, 77, 62, 57, 17, 31, 59, 66, 23, 52, 45, 67, 43, 70, 15, 86, 61, 44, 58, 95, 43, 59, 41, 1, 58, 14, 17, 52, 37, 76, 32, 33, 2, 77, 57, 68, 59, 33, 90, 10, 11, 59, 14, 54, 50, 90, 83, 55, 17, 88, 85, 28, 17, 8, 61, 15, 15, 15, 15, 13, 39, 95, 16, 44, 9, 36, 37, 33, 57, 57, 80, 61, 25, 50, 19, 22, 67, 84, 9, 50, 70, 20, 64, 14, 73, 86, 82, 15, 90, 27, 21, 75, 86, 25, 83, 14, 53, 38, 11, 50, 1, 84, 96, 10, 53, 24, 74, 26, 7, 5, 59, 59, 38, 80, 16, 86, 33, 86, 32, 39, 35, 88, 88, 21, 23, 88, 42, 72, 8, 39, 50, 38, 9, 84]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# convert labels to one-hot vectors\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isu7mtJyR2tb",
        "outputId": "bdfad8b8-2fc4-45d2-d0bf-cbd1d58d50f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(932, 97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "print(\"finished loading %d images from %d categories\"%(len(data), num_classes))\n",
        "print(\"train / validation / test split: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
        "print(\"training data shape: \", x_train.shape)\n",
        "print(\"training labels shape: \", y_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIXXClNmR8hY",
        "outputId": "c2c5e75c-f963-41d3-b9f4-e601e3008bbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished loading 6209 images from 97 categories\n",
            "train / validation / test split: 4346, 931, 932\n",
            "training data shape:  (4346, 224, 224, 3)\n",
            "training labels shape:  (4346, 97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "idx = [int(len(images) * random.random()) for i in range(8)]\n",
        "imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]\n",
        "concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.imshow(concat_image)"
      ],
      "metadata": {
        "id": "AN-FzKCQSBNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando Transfer Learning com base no treinamento VGG-16."
      ],
      "metadata": {
        "id": "aj8cN6lwTGjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "vgg.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-C2w1A8TQse",
        "outputId": "1b27b255-ed2e-4f02-c8c5-6d9b6f1f18b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 3s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138357544 (527.79 MB)\n",
            "Trainable params: 138357544 (527.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a reference to VGG's input layer\n",
        "inp = vgg.input\n",
        "\n",
        "# make a new softmax layer with num_classes neurons\n",
        "new_classification_layer = Dense(num_classes, activation='softmax')\n",
        "\n",
        "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
        "out = new_classification_layer(vgg.layers[-2].output)\n",
        "\n",
        "# create a new network between inp and out\n",
        "model_new = Model(inp, out)"
      ],
      "metadata": {
        "id": "nisDwxBMToN_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make all layers untrainable by freezing weights (except for last layer)\n",
        "for l, layer in enumerate(model_new.layers[:-1]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# ensure the last layer is trainable/not frozen\n",
        "for l, layer in enumerate(model_new.layers[-1:]):\n",
        "    layer.trainable = True\n",
        "\n",
        "model_new.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_new.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEPWXDxMTxdq",
        "outputId": "6b8ecc62-73f2-45d1-f97e-01f257b876cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 97)                397409    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134657953 (513.68 MB)\n",
            "Trainable params: 397409 (1.52 MB)\n",
            "Non-trainable params: 134260544 (512.16 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model_new.fit(x_train, y_train,\n",
        "                         batch_size=128,\n",
        "                         epochs=10,\n",
        "                         validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EtxdJ2jT1b2",
        "outputId": "e03adf11-8594-45dd-d3f7-192dfb5e2b5e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 1049s 31s/step - loss: 4.7842 - accuracy: 0.1240 - val_loss: 3.3529 - val_accuracy: 0.2900\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 1042s 31s/step - loss: 2.7964 - accuracy: 0.3909 - val_loss: 2.5834 - val_accuracy: 0.4447\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 1041s 31s/step - loss: 2.2847 - accuracy: 0.5067 - val_loss: 2.2324 - val_accuracy: 0.5027\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 1042s 31s/step - loss: 1.9703 - accuracy: 0.5771 - val_loss: 2.0183 - val_accuracy: 0.5532\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 1042s 31s/step - loss: 1.7690 - accuracy: 0.6173 - val_loss: 1.9068 - val_accuracy: 0.5811\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 1043s 31s/step - loss: 1.5941 - accuracy: 0.6505 - val_loss: 1.8037 - val_accuracy: 0.5983\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 1043s 31s/step - loss: 1.4679 - accuracy: 0.6802 - val_loss: 1.6400 - val_accuracy: 0.6412\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 1041s 31s/step - loss: 1.3424 - accuracy: 0.7186 - val_loss: 1.5698 - val_accuracy: 0.6391\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 1042s 31s/step - loss: 1.2408 - accuracy: 0.7312 - val_loss: 1.5100 - val_accuracy: 0.6563\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 1046s 31s/step - loss: 1.1582 - accuracy: 0.7547 - val_loss: 1.4523 - val_accuracy: 0.6531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validação do treinamento\n",
        "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4xJjLj_A-NT",
        "outputId": "7b9ac3f3-e0b2-4b57-c573-6a75fe18ea27"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.4933192729949951\n",
            "Test accuracy: 0.6652360558509827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, x = get_image('101_ObjectCategories/airplanes/image_0003.jpg')\n",
        "probabilities = model_new.predict([x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAIM6gPPBAtY",
        "outputId": "f3ac4aef-0f1b-4646-853c-75ca5703961e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 565ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Probabilidade de a imagem ser um das categorias\n",
        "probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npH6CPGJDPM0",
        "outputId": "6e56d5db-c172-471d-8fd0-14d9b618ea2a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.44915728e-11, 2.57987082e-01, 1.03953258e-20, 3.74713717e-16,\n",
              "        2.80400161e-14, 2.45713814e-08, 6.02143728e-14, 6.49768079e-16,\n",
              "        2.16008880e-06, 2.91468322e-10, 7.06454140e-09, 1.17143216e-04,\n",
              "        1.04748939e-10, 4.70629844e-15, 2.97361703e-17, 1.71529422e-07,\n",
              "        5.21905097e-09, 1.74954005e-06, 1.23597519e-03, 1.36041888e-06,\n",
              "        1.64220790e-11, 6.05337003e-10, 4.69719241e-09, 5.01263060e-07,\n",
              "        1.50354325e-08, 9.37369177e-18, 1.10231241e-10, 1.67742163e-13,\n",
              "        5.24197054e-15, 4.19510812e-15, 2.79329987e-21, 1.06889281e-14,\n",
              "        7.29431631e-05, 4.36498522e-04, 3.08127142e-08, 5.83553442e-13,\n",
              "        3.64376262e-09, 1.81844609e-10, 2.21137721e-02, 1.55086097e-10,\n",
              "        4.62383343e-12, 4.09941858e-09, 4.84706168e-14, 1.33936977e-04,\n",
              "        1.01731957e-22, 6.45534237e-10, 3.24011774e-17, 5.42071203e-14,\n",
              "        1.03902228e-14, 7.67111166e-11, 5.45231998e-17, 2.10775975e-15,\n",
              "        1.25090774e-13, 1.00703321e-04, 3.64595541e-11, 1.10516329e-09,\n",
              "        7.30233582e-12, 2.62371844e-16, 6.84523627e-07, 4.99866537e-21,\n",
              "        1.74236027e-06, 2.60144013e-08, 3.05791748e-15, 1.45514051e-07,\n",
              "        6.62644375e-12, 4.51339541e-13, 2.17928697e-09, 9.94385909e-17,\n",
              "        2.17150320e-09, 8.04421234e-07, 3.45203612e-17, 1.85062604e-14,\n",
              "        2.33443863e-12, 9.06052872e-11, 1.22552309e-20, 3.29290750e-09,\n",
              "        7.83014348e-11, 6.04367667e-11, 3.71598396e-09, 5.66140727e-07,\n",
              "        5.82997345e-05, 8.47339263e-20, 8.82269433e-15, 3.04729468e-13,\n",
              "        3.18250845e-16, 1.19925768e-04, 1.19195720e-09, 1.45072807e-08,\n",
              "        1.02077024e-17, 1.49863832e-16, 5.41787060e-10, 3.69505884e-15,\n",
              "        1.19578007e-17, 1.18848778e-10, 7.14292467e-01, 3.32124578e-03,\n",
              "        2.05583380e-11]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}